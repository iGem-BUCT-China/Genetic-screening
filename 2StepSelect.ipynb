{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "import GEOparse\n",
    "from typing import Tuple, List\n",
    "from sklearn.metrics import  roc_curve, auc, precision_recall_curve, confusion_matrix, classification_report, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import ast\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression, RidgeClassifier, Lasso, LassoCV, LogisticRegression, LogisticRegressionCV, ElasticNet,SGDClassifier,RidgeClassifierCV\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSE_Train = \"GSE162330\" # 用于训练的数据集\n",
    "Skip_Paint = True # 是否绘制不必要的绘图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [chi2, f_classif, mutual_info_classif]\n",
    "models = [\n",
    "    LinearRegression(), \n",
    "    RidgeClassifierCV(class_weight='balanced'),\n",
    "    LogisticRegression(),\n",
    "    Lasso(),\n",
    "    LinearSVC(\n",
    "        penalty='l1',\n",
    "        loss='squared_hinge',\n",
    "        dual=False,\n",
    "        C=0.01,\n",
    "        max_iter=10000,\n",
    "        multi_class='ovr',\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight='balanced',\n",
    "        verbose=0,\n",
    "        random_state=0,\n",
    "        tol=1e-4\n",
    "    ),\n",
    "    NuSVC(kernel='linear',\n",
    "        max_iter=10000)\n",
    "]\n",
    "n_features = [i for i in range(1, 16)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPL10558 = GEOparse.get_GEO(geo=\"GPL10558\", destdir=\"./datasets\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpl = GEOparse.get_GEO(geo='GPL10558', destdir=\"./datasets\", silent=True)\n",
    "# ./datasets/GPL10558.pkl\n",
    "gsedata = pd.read_pickle(\"./datasets/GSE162330.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsedata.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with missing values more than 50%\n",
    "gsedata = gsedata.dropna(axis=1, thresh=0.5*len(gsedata.index))\n",
    "gsedata = gsedata.dropna(axis=0, thresh=0.5*len(gsedata.columns))\n",
    "\n",
    "gsedata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsedata['characteristics: sample class'].value_counts() # 看一下感染这个列的分布"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数 准备"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义存储结果的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(columns=['method', 'model', 'n', 'acc', 'auc', 'f1', 'precision', 'recall', 'confusionmatrix','coef', 'coef_rounded', 'found_coef', 'multiple_times','intercept', 'gene_id', 'gene_symbol', 'gb_acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可接受的权重的范围筛选器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime_factors_only(n, allowed_factors):\n",
    "    for factor in allowed_factors:\n",
    "        while n % factor == 0:\n",
    "            n //= factor\n",
    "    return n == 1\n",
    "\n",
    "def get_allowed_numbers(max_n=25,allowed_factors=[2,3,5]):\n",
    "    result = []\n",
    "    for i in range(1, max_n + 1):\n",
    "        if is_prime_factors_only(i, allowed_factors):\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def is_all_in_list(list,n=25,allowed_factors=[2,3,5]):\n",
    "    for i in list:\n",
    "        if i not in get_allowed_numbers(n,allowed_factors):\n",
    "            return False\n",
    "    return list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画图"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分数分布图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distribution(ax, y_score, final_target):                                 # 这个函数负责处理分数分布图这一个小图\n",
    "    ax.scatter(range(len(y_score)), y_score, c=final_target, cmap='bwr', alpha=0.5)     # 画散点图，x轴是样本序号，y轴是分数，颜色是感染情况，颜色映射是蓝白红，透明度是0.5\n",
    "                                                                                        # x是样本序号：分开画，免得都黏在一起了，透明度的设置是一样的原因。\n",
    "    ax.set_title('分数分布图')\n",
    "    ax.set_xlabel('样本序号')\n",
    "    ax.set_ylabel('分数')\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']                                          # 中文乱码\n",
    "    plt.rcParams['axes.unicode_minus']=False                                            # 负号乱码\n",
    "    labels = ['细菌感染', '病毒感染']                                                   # 图例标签，这里是感染情况，以后如果换了的话得改。 #todo 将这个改为传入参数\n",
    "    ax.legend(labels, loc='upper right')                                                # 图例\n",
    "    ax.plot([0, len(y_score)], [0.5, 0.5], color='black', lw=1, linestyle='--')         # 以0.5为界，画一条虚线，表示分数大于0.5的是细菌感染，小于0.5的是病毒感染，这个以后也得改\n",
    "                                                                                        # 因为不同的模型（如linear regression 和 ridge regression）的判断标准不一样，所以这个界限也不一样。\n",
    "                                                                                        # 现在的做法是直接在y_score 上做手脚，还是……蛮不优雅的，但是先这样吧。\n",
    "                                                                                        # todo 将这个改为传入参数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(ax, y_score, final_target):\n",
    "    from sklearn.metrics import auc\n",
    "    fpr, tpr, thresholds = roc_curve(final_target, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax.set_xlabel('False Positive Rate 假阳性率')\n",
    "    ax.set_ylabel('True Positive Rate 真阳性率')\n",
    "    ax.set_title('ROC 曲线（受试者工作特征曲线）')\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(ax, y_score, final_target):\n",
    "    precision, recall, thresholds = precision_recall_curve(final_target, y_score)\n",
    "    ax.plot(recall, precision, color='darkorange', lw=2, label='PR curve')\n",
    "    ax.set_xlabel('Recall 召回率')\n",
    "    ax.set_ylabel('Precision 准确率')\n",
    "    ax.set_title('PR 曲线（准确率-召回率曲线）')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(ax, y_score, final_target):\n",
    "    y_pred = np.where(y_score > 0.5, 1, 0)\n",
    "    cnf_matrix = confusion_matrix(final_target, y_pred)\n",
    "    ax.imshow(cnf_matrix, interpolation='nearest',cmap=plt.cm.Blues) # type: ignore\n",
    "    ax.set_title('Confusion Matrix 混淆矩阵')\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['病毒感染', '细菌感染'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['病毒感染', '细菌感染'])\n",
    "    thresh = cnf_matrix.max() / 2.\n",
    "    for i in range(cnf_matrix.shape[0]):\n",
    "        for j in range(cnf_matrix.shape[1]):\n",
    "            plt.text(j, i, cnf_matrix[i, j],horizontalalignment=\"center\",color=\"white\" if cnf_matrix[i, j] > thresh else \"black\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label 感染情况')\n",
    "    plt.xlabel('Predicted label 预测情况')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分数分布图、ROC曲线、PR曲线和混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(score, target, title=\"分数分布图、ROC曲线、PR曲线和混淆矩阵\", feature=None, weight=None):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))                                                     # 规定画布大小为1200*1200px，分成2*2的4个小图\n",
    "    fig.suptitle(title, fontsize=16)                                                                    # 设置标题\n",
    "    plot_score_distribution(axs[0,0], score, target)                                                    # 第一个小图（左上角）是分数分布图 这里的axs[0,0]是指第一行第一列的小图，注意计数是从0开始的。\n",
    "    plot_roc_curve(axs[0,1], score, target)                                                             # 第二个小图（右上角）是ROC曲线\n",
    "    plot_pr_curve(axs[1,0], score, target)                                                              # 第三个小图（左下角）是PR曲线\n",
    "    plot_confusion_matrix(axs[1,1], score, target)                                                      # 第四个小图（右下角）是混淆矩阵\n",
    "    if feature or weight:                                                                               # 如果有feature或者weight的话，就在图的下方加上文字\n",
    "        try:\n",
    "            # add margin at bottom\n",
    "            fig.subplots_adjust(bottom=0.12)                                                            # 调整图的下边距，这样文字就不会挡住图了\n",
    "            # add feature and weight at the bottom\n",
    "            fig.text(0.5, 0.04, '使用feature:'+str(feature), ha='center', va='bottom', fontsize=12)     # 在图的下方加上文字，这里的0.5,0.04是指文字的位置，ha='center'是指文字居中，va='bottom'是指文字在底部，fontsize=12是指文字大小\n",
    "            fig.text(0.5, 0.02, '对应权重:'+str(weight), ha='center', va='bottom', fontsize=8)          # 同上\n",
    "        except:\n",
    "            pass                                                                                        # 如果出错了就算了\n",
    "    return fig, axs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 给定 基因 权重 和数据，计算分数并画图（可能需要修改）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testfeature(feature,weight,data,gpl=gpl,title=\"分数分布图、ROC曲线、PR曲线和混淆矩阵\"):\n",
    "    ID = []\n",
    "    for i in range(len(feature)):\n",
    "        ID.append(gpl.table[gpl.table['Gene Symbol'] == feature[i]]['ID'].values[0]) # type: ignore\n",
    "    ID = np.array(ID).reshape(-1)\n",
    "    testdata = data[ID]\n",
    "    n = len(testdata)\n",
    "    y_score = np.zeros(n)\n",
    "    y_pred = np.zeros(n)\n",
    "    for i in range(len(testdata)):\n",
    "        y_score [i] = np.dot(testdata.iloc[i],weight)\n",
    "        y_pred [i] = np.where(y_score[i] > 0.5, 1, 0)\n",
    "    try:\n",
    "        target = data['target'].values\n",
    "    except:\n",
    "        target = gsedata['target']\n",
    "    acc = accuracy_score(target, y_pred)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    fig, axs = plot_all(y_score, target, title, feature=feature, weight=weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重由放大得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testfeaturewithtimes(feature,weight,data,times=1,gpl=gpl,title=\"分数分布图、ROC曲线、PR曲线和混淆矩阵\"):\n",
    "    ID = []\n",
    "    for i in range(len(feature)):\n",
    "        ID.append(gpl.table[gpl.table['Gene Symbol'] == feature[i]]['ID'].values[0]) # type: ignore\n",
    "    ID = np.array(ID).reshape(-1)\n",
    "    testdata = data[ID]\n",
    "    n = len(testdata)\n",
    "    y_score = np.zeros(n)\n",
    "    y_pred = np.zeros(n)\n",
    "    for i in range(len(testdata)):\n",
    "        y_score [i] = np.dot(testdata.iloc[i],weight)\n",
    "        # 结果除以times\n",
    "        y_score [i] = y_score [i] / times\n",
    "        y_pred [i] = np.where(y_score[i] > 0.5, 1, 0)\n",
    "    try:\n",
    "        target = data['target'].values\n",
    "    except:\n",
    "        target = gsedata['target']\n",
    "    acc = accuracy_score(target, y_pred)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    fig, axs = plot_all(y_score, target, title, feature=feature, weight=weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画出按照倍数比例画出各种基因在不同条件下的表达量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(data_higher, gpl, gsedata, data_higher_times):\n",
    "    fig, ax = plt.subplots(1,50, figsize=(50,15))\n",
    "    for i in range(50):\n",
    "        sns.boxplot(\n",
    "            x='target', \n",
    "            y=data_higher.iloc[:,i],\n",
    "            data=gsedata,\n",
    "            ax=ax[i],\n",
    "            )\n",
    "        ax[i].set_title(gpl.table[gpl.table['ID'] == data_higher.columns[i]]['Gene Symbol'].values[0])\n",
    "\n",
    "        # 最上面加上对应的倍数\n",
    "        ax[i].text(0.1, 0.98,\n",
    "                   str('%.2f' % data_higher_times[i]),\n",
    "                   transform=ax[i].transAxes,\n",
    "                   fontsize=16,\n",
    "                   verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "        # 显示优化\n",
    "        ax[i].set_yscale('log') #对数坐标\n",
    "        ax[i].set_ylim(10, 40000) # y轴的范围\n",
    "        if i != 0: # 不显示x，y轴的标签， 不然太挤了没意义\n",
    "            ax[i].set_xlabel('')\n",
    "            ax[i].set_yticklabels([])\n",
    "            ax[i].set_ylabel('')\n",
    "            ax[i].set_yticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('')\n",
    "            ax[i].set_ylabel('表达量')\n",
    "        ax[i].tick_params(axis='x', rotation=90) # 不然画不下\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照n增加而计算准确率等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_n(ax,X_train, y_train, X_test, y_test, method, model, max_features=20):\n",
    "    train_acc,test_acc,train_roc,test_roc = [],[],[],[]\n",
    "    for i in range(1, max_features):\n",
    "        if method == 'mannual':\n",
    "            reselect = X_train.iloc[:, :i]\n",
    "            model.fit(reselect, y_train) # use the selected features to train the data\n",
    "            y_pred = model.predict(X_test.iloc[:, :i])\n",
    "            train_acc.append(model.score(reselect, y_train))\n",
    "            test_acc.append(model.score(X_test.iloc[:, :i], y_test))\n",
    "            train_roc.append(roc_auc_score(y_train, model.predict(reselect)))\n",
    "            test_roc.append(roc_auc_score(y_test, model.predict(X_test.iloc[:, :i])))\n",
    "        else:\n",
    "            reselect = SelectKBest(score_func=method, k=i)\n",
    "            reselect.fit(X_train, y_train)\n",
    "            model.fit(X_train.iloc[:, reselect.get_support(indices=True)], y_train) # use the selected features to train the data\n",
    "            y_pred = model.predict(X_test.iloc[:, reselect.get_support(indices=True)])\n",
    "            train_acc.append(model.score(X_train.iloc[:, reselect.get_support(indices=True)], y_train))\n",
    "            test_acc.append(model.score(X_test.iloc[:, reselect.get_support(indices=True)], y_test))\n",
    "            train_roc.append(roc_auc_score(y_train, model.predict(X_train.iloc[:, reselect.get_support(indices=True)])))\n",
    "            test_roc.append(roc_auc_score(y_test, model.predict(X_test.iloc[:, reselect.get_support(indices=True)])))\n",
    "    # ax.plot(range(1, max_features), train_acc, label='train_acc')\n",
    "    ax.plot(range(1, max_features), test_acc, label='test_acc')\n",
    "    # ax.plot(range(1, max_features), train_roc, label='train_roc')\n",
    "    ax.plot(range(1, max_features), test_roc, label='test_roc')\n",
    "    if method == 'mannual':\n",
    "        ax.set_title('使用'+str(model) + '按照表达比选择特征')\n",
    "    else:\n",
    "        ax.set_title('使用'+str(model) + '和' + method.__name__ + '选择特征')\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    ax.legend()\n",
    "    # grid\n",
    "    ax.grid()\n",
    "    # mark the maximum value\n",
    "    ax.scatter(test_acc.index(max(test_acc)) + 1, max(test_acc), marker='o', color='b')\n",
    "    ax.text(test_acc.index(max(test_acc)) + 1, max(test_acc), str(test_acc.index(max(test_acc)) + 1) + ', ' + str('%.2f%%' % (max(test_acc) * 100)))\n",
    "    ax.scatter(test_roc.index(max(test_roc)) + 1, max(test_roc), marker='o', color='r')\n",
    "    ax.text(test_roc.index(max(test_roc)) + 1, max(test_roc), str(test_roc.index(max(test_roc)) + 1) + ', ' + str('%.2f%%' % (max(test_roc) * 100)))\n",
    "    # y is 0.5 to 1\n",
    "    ax.set_ylim(0.5, 1)\n",
    "\n",
    "    if method != 'mannual':\n",
    "        select = SelectKBest(score_func=method, k=test_acc.index(max(test_acc)) + 1).fit(X_train, y_train)\n",
    "        X_train_new = select.transform(X_train)\n",
    "        X_test_new = select.transform(X_test)\n",
    "        model.fit(X_train_new, y_train)\n",
    "        y_pred = model.predict(X_test_new)\n",
    "        gene_id = X_train.columns[select.get_support(indices=True)]\n",
    "    else:\n",
    "        X_train_new = X_train.iloc[:, :test_acc.index(max(test_acc)) + 1]\n",
    "        X_test_new = X_test.iloc[:, :test_acc.index(max(test_acc)) + 1]\n",
    "        model.fit(X_train_new, y_train)\n",
    "        y_pred = model.predict(X_test_new)\n",
    "        gene_id = X_train_new.columns\n",
    "    gene_symbol = [gpl.table[gpl.table['ID'] == i]['Gene Symbol'].values[0] for i in gene_id] # type: ignore\n",
    "    # print('使用' + str(model) + '和' + method.__name__ + '选择特征后的特征为：', gene_symbol)\n",
    "    # print('使用' + str(model) + '和' + method.__name__ + '选择特征后的特征权重为：', model.coef_)\n",
    "    \n",
    "    n = test_acc.index(max(test_acc)) + 1\n",
    "    best_acc = max(test_acc)\n",
    "\n",
    "    return best_acc, n, gene_symbol, model.coef_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选哪些基因比较有差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_columns(data):\n",
    "    data_mean = data.groupby('target').mean()\n",
    "    higher_columns = []\n",
    "    higher_times = []\n",
    "    for i in range(data_mean.shape[1]):\n",
    "        if data_mean.iloc[1,i] > 2*data_mean.iloc[0,i] or 2*data_mean.iloc[0,i] < data_mean.iloc[1,i]:\n",
    "            higher_columns.append(data_mean.columns[i])\n",
    "            higher_times.append(data_mean.iloc[1,i]/data_mean.iloc[0,i])\n",
    "    return higher_columns, higher_times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "排序有差异的基因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_higher_columns(data, higher_columns, higher_times):\n",
    "    # sort the columns and times together\n",
    "    data_higher_times, data_higher_columns = zip(*sorted(zip(higher_times, higher_columns), reverse=True))\n",
    "    # convert data_higher_columns to list\n",
    "    data_higher_columns = list(data_higher_columns)\n",
    "    # add target column to the top of data_higher_columns\n",
    "    data_higher_columns.append('target')\n",
    "    # get the new data\n",
    "    data_higher = data[data_higher_columns]\n",
    "    return data_higher, data_higher_times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_acc(test_acc, n, gene_symbol, coef):\n",
    "    best_acc = max(test_acc)\n",
    "    best_i = 0\n",
    "    i = 0\n",
    "    print('最好的准确率为：', best_acc)\n",
    "    for acc in test_acc:\n",
    "        if acc == best_acc:\n",
    "            print('='*60)\n",
    "            print('对应的特征数为：', n[i],'方法为：', methods[i // len(models)].__name__,'特征为：', gene_symbol[i])\n",
    "            print('模型为：', models[i % len(models)],'特征权重为：', coef[i])\n",
    "            weights_avg = np.mean(coef[i])\n",
    "            new_coefs = []\n",
    "            for j in range(len(coef[i])):\n",
    "                # new_coefs.append(10*(coef[i][j]/weights_avg)) 保留2位小数\n",
    "                # new_coefs.append(round(10*(coef[i][j]/weights_avg), 2)) type numpy.ndarray doesn't define __round__ method\n",
    "                new_coefs.append(np.round(10*(coef[i][j]/weights_avg), 2))\n",
    "            print('比例大致为：', new_coefs)\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不记得了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbols(gene_symbols):\n",
    "    gene_symbols = gene_symbols.reset_index(drop=True)\n",
    "    symbol = []\n",
    "    for i in range(len(gene_symbols)):\n",
    "        for j in range(len(gene_symbols[i])):\n",
    "            if gene_symbols[i][j] not in symbol:\n",
    "                symbol.append(gene_symbols[i][j])\n",
    "            else:\n",
    "                pass\n",
    "    return symbol\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test all\n",
    "输入筛选方法和训练模型等 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: methods, models, n_features, X_train, X_test, y_train, y_test, gpl\n",
    "# output: results\n",
    "\n",
    "\n",
    "def test_all(methods, models, n_features, X_train, X_test, y_train, y_test, gpl):\n",
    "    import time\n",
    "    global final_results\n",
    "    for method in methods:\n",
    "        for n in n_features:\n",
    "            reselect = SelectKBest(score_func=method, k=n)\n",
    "            reselect.fit(X_train, y_train)\n",
    "            for model in models:\n",
    "                # log the time\n",
    "                time_start = time.time()\n",
    "                model.fit(reselect.transform(X_train), y_train)\n",
    "                y_score = model.predict(reselect.transform(X_test))\n",
    "                y_pred = np.array([1 if i > 0.5 else 0 for i in y_score], dtype=int)\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                auc = roc_auc_score(y_test, model.predict(reselect.transform(X_test)))\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                confusionmatrix = confusion_matrix(y_test, y_pred)\n",
    "                coef = model.coef_\n",
    "\n",
    "                weights_avg = np.mean(coef)\n",
    "                min_times = 0.5 / np.min(np.abs(coef)) if np.min(coef) != 0 else 0.5 / np.mean(np.abs(coef))\n",
    "                max_times = 25 / np.max(np.abs(coef))\n",
    "                multipletimes = np.linspace(min_times, max_times, num=1000)\n",
    "                found_coef = False\n",
    "                coef_roundeds = []\n",
    "                differences = []\n",
    "                for multipletime in multipletimes:\n",
    "                    coef_rounded = np.round(coef.ravel() * multipletime).tolist()\n",
    "                    try:\n",
    "                        coef_rounded = [int(i) for i in coef_rounded]\n",
    "                        abs_coef_rounded = [abs(i) for i in coef_rounded]\n",
    "                        if is_all_in_list(abs_coef_rounded):\n",
    "                            coef_roundeds.append(coef_rounded)\n",
    "                            difference = np.mean(np.abs(np.array(coef_rounded) - np.array(coef.ravel() * multipletime))) / np.mean(np.abs(np.array(coef_rounded)))\n",
    "                            differences.append(difference)\n",
    "                            found_coef = True\n",
    "                        else:\n",
    "                            pass\n",
    "                    except:\n",
    "                        pass\n",
    "                if not found_coef:\n",
    "                    coef_rounded = np.round(100 * coef.ravel() / weights_avg).tolist()\n",
    "                    try:\n",
    "                        coef_rounded = [int(i) for i in coef_rounded]\n",
    "                    except:\n",
    "                        pass\n",
    "                    multiple_time = 100\n",
    "                else:\n",
    "                    best_index = differences.index(min(differences))\n",
    "                    coef_rounded = coef_roundeds[best_index]\n",
    "                    multiple_time = np.mean(np.abs(coef_rounded)) / np.mean(np.abs(coef.ravel()))\n",
    "                    # is all > 0\n",
    "                    # if not all(i > 0 for i in coef_rounded):\n",
    "                    #     found_coef = False\n",
    "\n",
    "                intercept = model.intercept_\n",
    "                gene_id = X_train.columns[reselect.get_support(indices=True)]\n",
    "                gene_id = np.array(gene_id).reshape(-1)\n",
    "                gene_symbol = [gpl.table[gpl.table['ID'] == i]['Gene Symbol'].values[0] for i in gene_id]\n",
    "                gb_acc = [gpl.table[gpl.table['ID'] == i]['GB_ACC'].values[0] for i in gene_id]\n",
    "                final_results = final_results.append({\n",
    "                    'method': method.__name__,\n",
    "                    'model': model.__class__.__name__,\n",
    "                    'n': n,\n",
    "                    'acc': acc,\n",
    "                    'auc': auc,\n",
    "                    'f1': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'confusionmatrix': confusionmatrix,\n",
    "                    'coef': coef,\n",
    "                    'coef_rounded': coef_rounded,\n",
    "                    'found_coef': found_coef,\n",
    "                    'multiple_times': multiple_time,\n",
    "                    'intercept': intercept,\n",
    "                    'gene_id': gene_id,\n",
    "                    'gene_symbol': gene_symbol,\n",
    "                    'gb_acc': gb_acc\n",
    "                }, ignore_index=True)\n",
    "                time_end = time.time()\n",
    "                if time_end - time_start > 2:\n",
    "                    print('SLOW!!!:method:', method.__name__, 'n:', n, 'model:', model.__class__.__name__,'time cost', time_end - time_start, 's')\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_all_results 未使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: results, methods, models, gpl\n",
    "# output: figure\n",
    "def plot_all_results(results, methods, models, gpl):\n",
    "    # Group the results by method and model\n",
    "    groups = results.groupby(['method', 'model'])\n",
    "\n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axs = plt.subplots(len(methods), len(models) + 1, sharex=True, sharey=True, figsize=(5*len(models), 3*(len(methods)+1)))\n",
    "\n",
    "    # Iterate over the subplots and plot the acc and roc values\n",
    "    for i, method in enumerate(methods):\n",
    "        for j, model in enumerate(models):\n",
    "            group = groups.get_group((method.__name__, model.__class__.__name__))\n",
    "            axs[i, j].plot(group['n'], group['acc'], label='acc')\n",
    "            axs[i, j].plot(group['n'], group['auc'], label='auc')\n",
    "            axs[i, j].set_xlabel('n')\n",
    "            axs[i, j].set_ylabel('score')\n",
    "            axs[i, j].set_ylim([0.5, 1]) # Set y-axis limits\n",
    "            axs[i, j].legend()\n",
    "            # Find the maximum acc and roc values for this method and model\n",
    "            max_acc_idx = group['acc'].idxmax()\n",
    "            max_auc_idx = group['auc'].idxmax()\n",
    "            max_acc = group.loc[max_acc_idx, 'acc']\n",
    "            max_auc = group.loc[max_auc_idx, 'auc']\n",
    "            max_n_acc = group.loc[max_acc_idx, 'n']\n",
    "            max_n_auc = group.loc[max_auc_idx, 'n']\n",
    "\n",
    "            # Add annotations with the maximum acc and roc values\n",
    "            axs[i, j].annotate(f'Max Acc: {max_acc:.2f} (n={max_n_acc})',\n",
    "                                xy=(max_n_acc, max_acc),\n",
    "                                xytext=(max_n_acc, max_acc + 0.1),\n",
    "                                ha='center',\n",
    "                                arrowprops=dict(arrowstyle='->'))\n",
    "            axs[i, j].annotate(f'Max Roc: {max_auc:.2f} (n={max_n_auc})',\n",
    "                                xy=(max_n_auc, max_auc),\n",
    "                                xytext=(max_n_auc, max_auc - 0.1),\n",
    "                                ha='center',\n",
    "                                arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "            axs[i, j].set_title(f'{method.__name__} - {model.__class__.__name__}')\n",
    "            # grid\n",
    "            axs[i, j].grid()\n",
    "        # Create a table of gene symbols for the first model in the method's group\n",
    "        first_model = groups.get_group((method.__name__, models[0].__class__.__name__))\n",
    "        # gene_symbols = first_model['gene_symbol'].reset_index(drop=True)\n",
    "        # symbol = get_symbols(gene_symbols)\n",
    "        gene_ids = first_model['gene_id'].reset_index(drop=True)\n",
    "        id = get_symbols(gene_ids)\n",
    "        # cell_text = np.array(symbol).reshape(-1, 1)\n",
    "        cell_text = np.array([id]).T\n",
    "        row_labels = [f'Gene {i}' for i in range(len(id))]\n",
    "        col_labels = ['Gene Symbol', 'Gene ID']\n",
    "        table = axs[i, len(models)].table(cellText=cell_text, rowLabels=row_labels, colLabels=col_labels, loc='center')\n",
    "        axs[i, len(models)].axis('off')\n",
    "        axs[i, len(models)].set_title(f'{method.__name__} - selected genes')\n",
    "\n",
    "    # Iterate over the first column of subplots and set their y-axis labels to be the method names\n",
    "    for i, method in enumerate(methods):\n",
    "        axs[i, 0].set_ylabel(method.__name__)\n",
    "\n",
    "    # Add a main title to the figure\n",
    "    fig.suptitle('Accuracy and AOC by method and model')\n",
    "\n",
    "    # Adjust the layout of the subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定数据、选取基因和单向权重，计算分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score_with_feature(data, gpl, feature, weight, times, intercept,model):\n",
    "    ID = []\n",
    "    for i in range(len(feature)):\n",
    "        ID.append(gpl.table[gpl.table['Gene Symbol'] == feature[i]]['ID'].values[0])\n",
    "    ID = np.array(ID).reshape(-1)\n",
    "    ID.astype('str')\n",
    "    data = data[ID]\n",
    "    data.columns = feature\n",
    "    data_weight = data * weight\n",
    "    data_weight_sum = data_weight.sum(axis=1)\n",
    "    if model in ['LogisticRegression','LogisticRegressionCV','logisticregression','LR','lr']:\n",
    "        data_weight_z = data_weight_sum / times + intercept\n",
    "        data_weight_score = 1 / (1 + np.exp(-data_weight_z))\n",
    "    elif model in ['LinearRegression','RidgeClassifier','Lasso','LassoCV','ElasticNet']:\n",
    "        data_weight_score = data_weight_sum / times + intercept\n",
    "    elif model in ['NuSVC','LinearSVC','SVC']:\n",
    "        data_weight_score = data_weight_sum / times + intercept\n",
    "    else:\n",
    "        print('The model',model,'is not supported yet.')\n",
    "        data_weight_score = data_weight_sum / times + intercept\n",
    "    return data_weight_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次性完成基因筛选和模型训练\n",
    "通过调控C，使用LogisticRegression or LinearSVC 一次性完成基因筛选和模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf, method, model, X_train, y_train, X_test, y_test, params):\n",
    "    global final_results\n",
    "\n",
    "    acc_scores, auc_scores, f1_scores, precision_scores, recall_scores, n_features = [], [], [], [], [], []\n",
    "\n",
    "    for param in params:\n",
    "        clf.set_params(**param)\n",
    "        clf.fit(X_train, y_train)\n",
    "        nonzero_coef_indices = np.flatnonzero(clf.coef_)  # Indices where coef_ != 0\n",
    "        n = len(nonzero_coef_indices)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred = np.array([1 if i > 0.5 else 0 for i in y_pred], dtype=int)\n",
    "        \n",
    "        if 3 <= n <= 45:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            acc = clf.score(X_test, y_test)\n",
    "\n",
    "            coef = clf.coef_\n",
    "            # only use non-zero coef\n",
    "            coef = coef[coef != 0]\n",
    "            weights_avg = np.mean(coef)\n",
    "            min_times = 0.5 / np.min(np.abs(coef)) if np.min(coef) != 0 else 0.5 / np.mean(np.abs(coef))\n",
    "            max_times = 25 / np.max(np.abs(coef))\n",
    "            multipletimes = np.linspace(min_times, max_times, num=1000)\n",
    "            found_coef = False\n",
    "            coef_roundeds = []\n",
    "            differences = []\n",
    "            for multipletime in multipletimes:\n",
    "                coef_rounded = np.round(coef.ravel() * multipletime).tolist()\n",
    "                try:\n",
    "                    coef_rounded = [int(i) for i in coef_rounded]\n",
    "                    abs_coef_rounded = [abs(i) for i in coef_rounded]\n",
    "                    if is_all_in_list(abs_coef_rounded):\n",
    "                        coef_roundeds.append(coef_rounded)\n",
    "                        difference = np.mean(np.abs(np.array(coef_rounded) - np.array(coef.ravel() * multipletime))) / np.mean(np.abs(np.array(coef_rounded)))\n",
    "                        differences.append(difference)\n",
    "                        found_coef = True\n",
    "                        times = multipletime\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    pass\n",
    "            if not found_coef:\n",
    "                coef_rounded = np.round(100 * coef.ravel() / weights_avg).tolist()\n",
    "                try:\n",
    "                    coef_rounded = [int(i) for i in coef_rounded]\n",
    "                except:\n",
    "                    pass\n",
    "                times = 100\n",
    "\n",
    "            intercept = clf.intercept_\n",
    "\n",
    "            nonzero_coef_indices = np.flatnonzero(clf.coef_) # get the indices of non-zero coef\n",
    "            gene_id = list(X_train.columns[nonzero_coef_indices])\n",
    "            gene_symbol = list(X_train.columns[nonzero_coef_indices])\n",
    "            gb_acc = list(X_train.columns[nonzero_coef_indices])\n",
    "\n",
    "            result = pd.DataFrame({\n",
    "                'method': [method],\n",
    "                'model': [model],\n",
    "                'n': [n],\n",
    "                'acc': [acc],\n",
    "                'auc': [auc],\n",
    "                'f1': [f1],\n",
    "                'precision': [precision],\n",
    "                'recall': [recall],\n",
    "                'confusionmatrix': [cm],\n",
    "                'coef': [coef.tolist()],\n",
    "                'coef_rounded': [coef_rounded],\n",
    "                'found_coef': [found_coef],\n",
    "                'multiple_times': [times],\n",
    "                'intercept': [intercept.item()],\n",
    "                'gene_id': [gene_id],\n",
    "                'gene_symbol': [gene_symbol],\n",
    "                'gb_acc': [gb_acc]\n",
    "            })\n",
    "\n",
    "            final_results = pd.concat([final_results, result], ignore_index=True) \n",
    "\n",
    "            # For plotting\n",
    "            n_features.append(n) \n",
    "            auc_scores.append(auc)\n",
    "            f1_scores.append(f1)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "    # Plotting inside the function\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(n_features, auc_scores, label='AUC')\n",
    "    plt.plot(n_features, f1_scores, label='F1 score')\n",
    "    plt.plot(n_features, precision_scores, label='Precision', linestyle='dashed')\n",
    "    plt.plot(n_features, recall_scores, label='Recall', linestyle='dashed')\n",
    "    plt.title('Performance metrics by number of features (' + method + ')')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('Performance metrics')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只保留细菌和病毒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {\n",
    "    'virus': 1,\n",
    "    'bacteria': 0,\n",
    "    'viral': 1,\n",
    "    'bacterial': 0,\n",
    "    'S. aureus, MRSA': 0,\n",
    "    'Influenza A': 1,\n",
    "    'E. coli': 0,\n",
    "    'S. pneumoniae': 0,\n",
    "    'S. aureus, MSSA': 0,\n",
    "    'None': -1,\n",
    "    'Influenza B': 1,\n",
    "    'non-infectious illness': -1,\n",
    "    'v': 1,\n",
    "    'b': 0,\n",
    "    'c': -1\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 病毒与未感染的鉴别\n",
    "train_data = gsedata.copy()\n",
    "# prevent Object of type \"Series[Unknown]\" is not callable\n",
    "train_data['target'] = train_data['characteristics: sample class'].map(target_map)\n",
    "train_data = train_data[train_data['target'] != -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选出 细菌病毒感染中表达量大差异于等于2倍的基因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_columns, higher_times = get_higher_columns(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据倍率不同进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_higher, data_higher_times = sort_higher_columns(train_data, higher_columns, higher_times)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_higher.drop(['target'], axis=1), data_higher['target'], test_size=0.2, random_state=42)\n",
    "# X_train, y_train = sm.fit_resample(data_higher, data_higher['target'])\n",
    "# smote\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "X_test, y_test = sm.fit_resample(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练不同的模型，使用不同的特征选择方法，选择出特征后，使用不同的方法来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(columns=['method', 'model', 'n', 'acc', 'auc', 'f1', 'precision', 'recall', 'confusionmatrix','coef', 'coef_rounded', 'found_coef', 'multiple_times','intercept', 'gene_id', 'gene_symbol', 'gb_acc'])\n",
    "max_run_num = 300\n",
    "C_values = np.logspace(-9, 0, num=max_run_num, base=10)\n",
    "alpha_values = np.logspace(-5, 15, num=max_run_num, base=10)\n",
    "\n",
    "lsvc = LinearSVC(penalty=\"l1\", dual=False)\n",
    "lsvc_params = [{'C': C} for C in C_values]\n",
    "evaluate_classifier(lsvc, 'LinearSVC', 'lsvc', X_train, y_train, X_test, y_test, lsvc_params)\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_params = [{'C': C} for C in C_values]\n",
    "evaluate_classifier(lr, 'LogisticRegression', 'lr', X_train, y_train, X_test, y_test, lr_params)\n",
    "lasso = Lasso(alpha=0.1, max_iter=5000)\n",
    "lasso_params = [{'alpha': alpha} for alpha in alpha_values]\n",
    "evaluate_classifier(lasso, 'lasso', 'lasso', X_train, y_train, X_test, y_test, lasso_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic-Net\n",
    "en = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000)\n",
    "en_params = [{'alpha': alpha, 'l1_ratio': l1_ratio} for alpha in alpha_values for l1_ratio in np.linspace(0, 1, num=5)]\n",
    "evaluate_classifier(en, 'ElasticNet', 'en', X_train, y_train, X_test, y_test, en_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制 'final_results' 到 'final_results_unique' \n",
    "final_results_unique = final_results.copy()\n",
    "\n",
    "# 将 'coef_rounded' 和 'gene_id' 列转化为字符串\n",
    "final_results_unique['coef_rounded'] = final_results_unique['coef_rounded'].astype('str')\n",
    "final_results_unique['gene_id'] = final_results_unique['gene_id'].astype('str')\n",
    "\n",
    "# 删除重复行\n",
    "final_results_unique.drop_duplicates(subset=['coef_rounded', 'gene_id'], keep='first', inplace=True)\n",
    "final_results_unique['coef_rounded'] = final_results_unique['coef_rounded'].apply(ast.literal_eval)\n",
    "final_results_unique['gene_id'] = final_results_unique['gene_id'].apply(ast.literal_eval)\n",
    "final_results_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_columns = ['found_coef','auc', 'recall', 'n']\n",
    "sort_order = [False,False, False, True]\n",
    "\n",
    "# Sort the DataFrame\n",
    "results_sorted = final_results_unique.sort_values(by=sort_columns, ascending=sort_order).reset_index(drop=True).copy()\n",
    "results_sorted_view = results_sorted.head(5).copy().drop(['coef','gene_id','intercept','multiple_times'], axis=1)\n",
    "results_sorted_view\n",
    "\n",
    "# todo: 当连续几个的gb_acc和coef_rounded都和上一个一样时，不用再显示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce\n",
    "results_sorted_found_coef = results_sorted[results_sorted['found_coef'] == True].reset_index(drop=True).copy()\n",
    "reperduce = []\n",
    "for i in range(len(results_sorted_found_coef)):\n",
    "    gene_ids = results_sorted_found_coef['gene_id'][i]\n",
    "    # check if gene_ids are in X_test columns\n",
    "    if set(gene_ids).issubset(set(X_test.columns)):\n",
    "        X_test_selected = X_test[gene_ids].copy()\n",
    "    else:\n",
    "        # modify gene_ids to match X_test columns\n",
    "        gene_ids = [col for col in gene_ids if col in X_test.columns]\n",
    "        X_test_selected = X_test[gene_ids].copy()\n",
    "    y_test_selected = y_test\n",
    "    # calculate score\n",
    "    coef = results_sorted_found_coef['coef_rounded'][i]\n",
    "    times = results_sorted_found_coef['multiple_times'][i]\n",
    "    intercept = results_sorted_found_coef['intercept'][i]\n",
    "    model_name = results_sorted_found_coef['model'][i]\n",
    "\n",
    "    scores = []\n",
    "    predictions = []\n",
    "    \n",
    "    # determine model and calculate scores and predictions\n",
    "    if model_name == 'LinearRegression':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'RidgeClassifier' or model_name == 'RidgeClassifierCV':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'LogisticRegression' or model_name == 'LogisticRegressionCV':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            z = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            score = 1 / (1 + np.exp(-z))\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'Lasso' or model_name == 'LassoCV':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[[j]], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'LinearSVC' or model_name == 'NuSVC':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'ElasticNet':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[[j]], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'SGDClassifier':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    else:\n",
    "        # handle SVC\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "    # calculate evaluation metrics\n",
    "    acc = accuracy_score(y_test_selected, predictions)\n",
    "    auc = roc_auc_score(y_test_selected, scores)\n",
    "    f1 = f1_score(y_test_selected, predictions)\n",
    "    precision = precision_score(y_test_selected, predictions)\n",
    "    recall = recall_score(y_test_selected, predictions)\n",
    "\n",
    "    # 能复现\n",
    "    can_reproduce = True\n",
    "\n",
    "    # if not acc == results_sorted_found_coef['acc'][i]:\n",
    "    #     can_reproduce = False\n",
    "    #     # print results\n",
    "    #     print('Model:', model_name, 'n:', results_sorted_found_coef['n'][i],'acc:', acc, 'auc:', auc, 'f1:', f1, 'precision:', precision, 'recall:', recall)\n",
    "    #     print(acc)\n",
    "    #     print(results_sorted_found_coef['acc'][i])\n",
    "    \n",
    "    reperduce.append(can_reproduce)\n",
    "\n",
    "results_sorted_found_coef['reproduce'] = reperduce\n",
    "results_sorted_found_coef_view = results_sorted_found_coef[results_sorted_found_coef['reproduce'] == True].copy().drop(['coef','gene_id','intercept','multiple_times'], axis=1)\n",
    "results_sorted_found_coef_view.head(5)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择的模型\n",
    "best_model = results_sorted_found_coef.iloc[0].copy()\n",
    "weight = best_model.coef_rounded\n",
    "feature =  best_model.gene_symbol\n",
    "times = best_model.multiple_times\n",
    "intercept = best_model.intercept\n",
    "model = best_model.model\n",
    "\n",
    "# 实际输入的模型\n",
    "print('feature:', feature)\n",
    "print('weight:', weight)\n",
    "\n",
    "# up means weight > 0\n",
    "# down means weight < 0\n",
    "up_feature = []\n",
    "up_weight = []\n",
    "down_feature = []\n",
    "down_weight = []\n",
    "for i in range(len(weight)):\n",
    "    if weight[i] > 0:\n",
    "        up_feature.append(feature[i])\n",
    "        up_weight.append(weight[i])\n",
    "    else:\n",
    "        down_feature.append(feature[i])\n",
    "        down_weight.append(-1*weight[i])\n",
    "up_times = times\n",
    "down_times = times\n",
    "up_intercept = best_model.intercept\n",
    "down_intercept = best_model.intercept\n",
    "up_model = best_model.intercept\n",
    "down_model = best_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_score = cal_score_with_feature(gsedata, gpl, up_feature, up_weight, up_times, up_intercept, model=up_model)\n",
    "down_score = cal_score_with_feature(gsedata, gpl, down_feature, down_weight, down_times, down_intercept, model=down_model)\n",
    "target = gsedata['target']\n",
    "merge = pd.concat([up_score, down_score, target], axis=1)\n",
    "merge.columns = ['up', 'down', 'target']\n",
    "merge_by_target = merge.groupby('target')\n",
    "# the dict of the target and how many times it appears\n",
    "target_dict = dict(merge_by_target.size())\n",
    "\n",
    "# where the target should be\n",
    "# non-infectious illness：up和down都小于0.5\n",
    "# bacterial：down大于0.5，且up<down\n",
    "# viral：up大于0.5，且up>down\n",
    "conditions = [\n",
    "    (merge['up'] < 0.5) & (merge['down'] < 0.5),\n",
    "    (merge['up'] < 0.5) & (merge['down'] > 0.5),\n",
    "    (merge['up'] > 0.5) & (merge['down'] < 0.5),\n",
    "    (merge['up'] > 0.5) & (merge['down'] > 0.5) & (merge['up'] > merge['down']),\n",
    "    (merge['up'] > 0.5) & (merge['down'] > 0.5) & (merge['up'] < merge['down'])\n",
    "]\n",
    "place_for_text = [\n",
    "    (0.25, 0.25),\n",
    "    (0.25, 0.75),\n",
    "    (0.75, 0.25),\n",
    "    # (0.75, 0.75), should be righter and lower\n",
    "    (0.9, 0.6),\n",
    "    (0.6, 0.9)\n",
    "]\n",
    "target_map = [\n",
    "    'non-infectious illness',\n",
    "    'bacterial',\n",
    "    'viral',\n",
    "    'viral',\n",
    "    'bacterial'\n",
    "]\n",
    "merge['predict'] = np.select(conditions, target_map)\n",
    "merge['right'] = merge['target'] == merge['predict']\n",
    "merge['area'] = np.select(conditions, ['1', '2', '3', '4', '5'])\n",
    "\n",
    "# accuracy\n",
    "accuracy_score(merge['target'], merge['predict'])\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "# sns.scatterplot(data=merge, x='up', y='down', hue='target', style='right', s=100) # style='right' will make those True points drawn as x. Should fix it.\n",
    "sns.scatterplot(data=merge, x='up', y='down', hue='target', style='right',style_order=[True,False], s=30, size='right', sizes=[100, 30])\n",
    "plt.plot([0.5, 1], [0.5, 1], color='red', linewidth=2)\n",
    "# x=0.5, y 0~0.5\n",
    "plt.plot([0.5, 0.5], [0, 0.5], color='red', linewidth=2)\n",
    "# x=0.5~1, y=0.5\n",
    "plt.plot([0, 0.5], [0.5, 0.5], color='red', linewidth=2)\n",
    "# x=0.5~1, y=0.5~1, width=0.5, \n",
    "plt.plot([0.5, 1], [0.5, 0.5], color='red', linewidth=0.5, linestyle='dotted')\n",
    "plt.plot([0.5, 0.5], [0.5, 1], color='red', linewidth=0.5, linestyle='dotted')\n",
    "\n",
    "# plot the area\n",
    "merge_by_area = merge.groupby('area')\n",
    "for area in ['1', '2', '3', '4', '5']:\n",
    "    try:\n",
    "        area_data = merge_by_area.get_group(area)\n",
    "        area_data_confusion_matrix = confusion_matrix(area_data['target'], area_data['predict'])\n",
    "        text = f'Area {area}\\n'\n",
    "        for i in range(len(area_data_confusion_matrix)):\n",
    "            for j in range(len(area_data_confusion_matrix)):\n",
    "                text += f'|{area_data_confusion_matrix[i][j]}'\n",
    "            text += '|\\n'\n",
    "        plt.text(place_for_text[int(area)-1][0], place_for_text[int(area)-1][1], text, fontsize=12)\n",
    "    except:\n",
    "        # plt.text(place_for_text[int(area)-1][0], place_for_text[int(area)-1][1], f'Area {area}\\n0\\n0\\n0', fontsize=12)\n",
    "        pass\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "# 在右边显示各个target的数量\n",
    "for i in range(len(target_dict)):\n",
    "    plt.text(3.5, 0.7 - 0.08 * i, f'{list(target_dict.keys())[i]}: {list(target_dict.values())[i]}', fontsize=12)\n",
    "    # 实际被预测为该target的数量\n",
    "    # plt.text(1.11, 1.01 - 0.08 * i, f'Predicted: {len(merge[merge[\"predict\"] == list(target_dict.keys())[i]])}, Accuracy: {accuracy_score(merge[merge[\"target\"] == list(target_dict.keys())[i]][\"target\"], merge[merge[\"target\"] == list(target_dict.keys())[i]][\"predict\"])}', fontsize=12)\n",
    "    # acc only keep 2 digits and use %\n",
    "    plt.text(3.5, 0.66 - 0.08 * i, f'Predicted: {len(merge[merge[\"predict\"] == list(target_dict.keys())[i]])}, Accuracy: {round(accuracy_score(merge[merge[\"target\"] == list(target_dict.keys())[i]][\"target\"], merge[merge[\"target\"] == list(target_dict.keys())[i]][\"predict\"]), 2) * 100}%', fontsize=12)\n",
    "# confusion matrix , also plot the text\n",
    "confusion_matrix_text = confusion_matrix(merge['target'], merge['predict'])\n",
    "text = 'Confusion Matrix\\n'\n",
    "for i in range(len(confusion_matrix_text)):\n",
    "    for j in range(len(confusion_matrix_text)):\n",
    "        text += f'|{confusion_matrix_text[i][j]}'\n",
    "    text += '|'\n",
    "    text += '\\n'\n",
    "plt.text(3.5, 0.3, text, fontsize=12)\n",
    "\n",
    "# accuracy\n",
    "plt.text(3.5, 0.2, f'Accuracy: {accuracy_score(merge[\"target\"], merge[\"predict\"])}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_data\n",
    "test_data['target'] = test_data['target'].map(target_map)\n",
    "\n",
    "merge_2 = train_data.copy()\n",
    "\n",
    "viral_score = merge_2['up']\n",
    "bacterial_score = merge_2['down']\n",
    "target = train_data\n",
    "# score is  viral_score - bacterial_score + 0.5\n",
    "# viral_score,  bacterial_score and score are list.\n",
    "score = np.array(viral_score) - np.array(bacterial_score) + 0.5\n",
    "\n",
    "plot_all(\n",
    "    score, \n",
    "    target,\n",
    "    title=\"分数分布图、ROC曲线、PR曲线和混淆矩阵\",\n",
    "    feature=feature,\n",
    "    weight=weight\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文献的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文献中模型\n",
    "# 病毒高的\n",
    "up_feature =  ['IFIT1', 'TRDV3', 'SIGLEC1','LY6E']\n",
    "up_weight = [1,1 ,4, 4]\n",
    "up_times = 19000\n",
    "up_intercept = -0.5\n",
    "up_model = 'LogisticRegression'\n",
    "# 细菌高的\n",
    "down_feature = ['ARG1', 'CD177', 'VNN1']\n",
    "down_weight = [1, 4, 5]\n",
    "down_times = 30000\n",
    "down_intercept = -0.5\n",
    "down_model = 'LogisticRegression'\n",
    "# 实际输入的模型\n",
    "feature =  up_feature + down_feature\n",
    "print('feature:', feature)\n",
    "weight = up_weight + (np.array(down_weight) * -1).tolist()\n",
    "print('weight:', weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_score = cal_score_with_feature(gsedata, gpl, up_feature, up_weight, up_times, up_intercept, model=up_model)\n",
    "down_score = cal_score_with_feature(gsedata, gpl, down_feature, down_weight, down_times, down_intercept, model=down_model)\n",
    "target = gsedata['target']\n",
    "merge = pd.concat([up_score, down_score, target], axis=1)\n",
    "merge.columns = ['up', 'down', 'target']\n",
    "merge_by_target = merge.groupby('target')\n",
    "# the dict of the target and how many times it appears\n",
    "target_dict = dict(merge_by_target.size())\n",
    "\n",
    "# where the target should be\n",
    "# non-infectious illness：up和down都小于0.5\n",
    "# bacterial：down大于0.5，且up<down\n",
    "# viral：up大于0.5，且up>down\n",
    "conditions = [\n",
    "    (merge['up'] < 0.5) & (merge['down'] < 0.5),\n",
    "    (merge['up'] < 0.5) & (merge['down'] > 0.5),\n",
    "    (merge['up'] > 0.5) & (merge['down'] < 0.5),\n",
    "    (merge['up'] > 0.5) & (merge['down'] > 0.5) & (merge['up'] > merge['down']),\n",
    "    (merge['up'] > 0.5) & (merge['down'] > 0.5) & (merge['up'] < merge['down'])\n",
    "]\n",
    "place_for_text = [\n",
    "    (0.25, 0.25),\n",
    "    (0.25, 0.75),\n",
    "    (0.75, 0.25),\n",
    "    # (0.75, 0.75), should be righter and lower\n",
    "    (0.9, 0.6),\n",
    "    (0.6, 0.9)\n",
    "]\n",
    "target_map = [\n",
    "    'non-infectious illness',\n",
    "    'bacterial',\n",
    "    'viral',\n",
    "    'viral',\n",
    "    'bacterial'\n",
    "]\n",
    "merge['predict'] = np.select(conditions, target_map)\n",
    "merge['right'] = merge['target'] == merge['predict']\n",
    "merge['area'] = np.select(conditions, ['1', '2', '3', '4', '5'])\n",
    "\n",
    "# accuracy\n",
    "accuracy_score(merge['target'], merge['predict'])\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "# sns.scatterplot(data=merge, x='up', y='down', hue='target', style='right', s=100) # style='right' will make those True points drawn as x. Should fix it.\n",
    "sns.scatterplot(data=merge, x='up', y='down', hue='target', style='right',style_order=[True,False], s=30, size='right', sizes=[100, 30])\n",
    "plt.plot([0.5, 1], [0.5, 1], color='red', linewidth=2)\n",
    "# x=0.5, y 0~0.5\n",
    "plt.plot([0.5, 0.5], [0, 0.5], color='red', linewidth=2)\n",
    "# x=0.5~1, y=0.5\n",
    "plt.plot([0, 0.5], [0.5, 0.5], color='red', linewidth=2)\n",
    "# x=0.5~1, y=0.5~1, width=0.5, \n",
    "plt.plot([0.5, 1], [0.5, 0.5], color='red', linewidth=0.5, linestyle='dotted')\n",
    "plt.plot([0.5, 0.5], [0.5, 1], color='red', linewidth=0.5, linestyle='dotted')\n",
    "\n",
    "# plot the area\n",
    "merge_by_area = merge.groupby('area')\n",
    "for area in ['1', '2', '3', '4', '5']:\n",
    "    try:\n",
    "        area_data = merge_by_area.get_group(area)\n",
    "        area_data_confusion_matrix = confusion_matrix(area_data['target'], area_data['predict'])\n",
    "        text = f'Area {area}\\n'\n",
    "        for i in range(len(area_data_confusion_matrix)):\n",
    "            for j in range(len(area_data_confusion_matrix)):\n",
    "                text += f'|{area_data_confusion_matrix[i][j]}'\n",
    "            text += '|\\n'\n",
    "        plt.text(place_for_text[int(area)-1][0], place_for_text[int(area)-1][1], text, fontsize=12)\n",
    "    except:\n",
    "        # plt.text(place_for_text[int(area)-1][0], place_for_text[int(area)-1][1], f'Area {area}\\n0\\n0\\n0', fontsize=12)\n",
    "        pass\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "# 在右边显示各个target的数量\n",
    "for i in range(len(target_dict)):\n",
    "    plt.text(1.1, 0.7 - 0.08 * i, f'{list(target_dict.keys())[i]}: {list(target_dict.values())[i]}', fontsize=12)\n",
    "    # 实际被预测为该target的数量\n",
    "    # plt.text(1.11, 1.01 - 0.08 * i, f'Predicted: {len(merge[merge[\"predict\"] == list(target_dict.keys())[i]])}, Accuracy: {accuracy_score(merge[merge[\"target\"] == list(target_dict.keys())[i]][\"target\"], merge[merge[\"target\"] == list(target_dict.keys())[i]][\"predict\"])}', fontsize=12)\n",
    "    # acc only keep 2 digits and use %\n",
    "    plt.text(1.1, 0.66 - 0.08 * i, f'Predicted: {len(merge[merge[\"predict\"] == list(target_dict.keys())[i]])}, Accuracy: {round(accuracy_score(merge[merge[\"target\"] == list(target_dict.keys())[i]][\"target\"], merge[merge[\"target\"] == list(target_dict.keys())[i]][\"predict\"]), 2) * 100}%', fontsize=12)\n",
    "# confusion matrix , also plot the text\n",
    "confusion_matrix_text = confusion_matrix(merge['target'], merge['predict'])\n",
    "text = 'Confusion Matrix\\n'\n",
    "for i in range(len(confusion_matrix_text)):\n",
    "    for j in range(len(confusion_matrix_text)):\n",
    "        text += f'|{confusion_matrix_text[i][j]}'\n",
    "    text += '|'\n",
    "    text += '\\n'\n",
    "plt.text(1.1, 0.3, text, fontsize=12)\n",
    "\n",
    "# accuracy\n",
    "plt.text(1.1, 0.2, f'Accuracy: {accuracy_score(merge[\"target\"], merge[\"predict\"])}', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gsedata[gsedata['target'].isin(['viral', 'bacterial'])]\n",
    "test_data['target'] = test_data['target'].map({'viral': 1, 'bacterial': 0})\n",
    "\n",
    "merge_2 = merge[merge['target'] != 'non-infectious illness'].copy()\n",
    "\n",
    "viral_score = merge_2['up']\n",
    "bacterial_score = merge_2['down']\n",
    "target = merge_2['target'].map({'viral': 1, 'bacterial': 0})\n",
    "\n",
    "# score is  viral_score - bacterial_score + 0.5\n",
    "# viral_score,  bacterial_score and score are list.\n",
    "score = np.array(viral_score) - np.array(bacterial_score) + 0.5\n",
    "\n",
    "plot_all(\n",
    "    score, \n",
    "    target,\n",
    "    title=\"分数分布图、ROC曲线、PR曲线和混淆矩阵\",\n",
    "    feature=feature,\n",
    "    weight=weight\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
