{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变量设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集名称，对应 https://xenabrowser.net/datapages/ 中缩写\n",
    "dataset_name =  ['LUAD', 'LUSC']\n",
    "# 设置俩： ['LUAD', 'LUSC']\n",
    "# 更多更多也行\n",
    "# 单一个： ['LUAD'] \n",
    "# All possible Value:\n",
    "# ['ACC','CHOL','BLCA','BRCA','CESC','COAD','UCEC','ESCA','GBM','HNSC','KICH','KIRC','KIRP','DLBC','LIHC','LGG','LUAD','LUSC','SKCM','MESO','UVM','OV','PAAD','PCPG','PRAD','READ','SARC','STAD','TGCT','THYM','THCA','UCS']:\n",
    "method = 'normal' # diff是上面俩数据集的差异，normal是和正常样本的差异\n",
    "# method = 'diff'\n",
    "loglevel = ''\n",
    "# loglevel = 'debug'\n",
    "\n",
    "balance_input = 'true' # or true \n",
    "rate1 = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "import GEOparse\n",
    "from typing import Tuple, List\n",
    "from sklearn.metrics import  roc_curve, auc, precision_recall_curve, confusion_matrix, classification_report, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix,fbeta_score\n",
    "from imblearn.over_sampling import SMOTE, SMOTEN\n",
    "sm = SMOTEN(random_state=42)\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression, RidgeClassifier, Lasso, LassoCV, LogisticRegression, LogisticRegressionCV, ElasticNet,ElasticNetCV,SGDClassifier,RidgeClassifierCV\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import pearsonr\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_parse_data(dataset_name):\n",
    "    pkl_file_path = './datasets/' + 'TCGA-' + dataset_name + '.mirna_transposed.pkl'\n",
    "    if os.path.exists(pkl_file_path):\n",
    "        df_t = pd.read_pickle(pkl_file_path)\n",
    "    else:\n",
    "        url = \"https://gdc-hub.s3.us-east-1.amazonaws.com/download/TCGA-\" + dataset_name + \".mirna.tsv.gz\"\n",
    "        response = requests.get(url)\n",
    "        os.makedirs('./datasets', exist_ok=True)\n",
    "        gz_file_path = './datasets/' + 'TCGA-' + dataset_name + '.mirna.tsv.gz'\n",
    "        with open(gz_file_path, 'wb') as f_out:\n",
    "            f_out.write(response.content)\n",
    "        tsv_file_path = './datasets/' + 'TCGA-' + dataset_name + '.mirna.tsv'\n",
    "        with gzip.open(gz_file_path, 'rb') as f_in:\n",
    "            with open(tsv_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "        df_t = df.transpose()\n",
    "        df_t.columns = df_t.iloc[0]\n",
    "        df_t = df_t.drop(df_t.index[0])\n",
    "        df_t = np.power(2, df_t) - 1\n",
    "        df_t = df_t.round(4)\n",
    "        df_t['Status'] = df_t.index.map(lambda x: 1 if x.split('-')[3][:2] == '01' else 0)\n",
    "        df_t = df_t[['Status'] + [col for col in df_t.columns if col != 'Status']]\n",
    "        df_t.to_pickle(pkl_file_path)\n",
    "        os.remove(gz_file_path)\n",
    "        os.remove(tsv_file_path)\n",
    "    df_t = df_t.apply(pd.to_numeric)\n",
    "    return df_t\n",
    "\n",
    "def find_healthy_samples(df):\n",
    "    healthy_samples_df = df[df['Status'] == 0]\n",
    "    return healthy_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance_data\n",
    "def getStatus0data():\n",
    "    Status0data = pd.DataFrame()\n",
    "    Status1data = pd.DataFrame()\n",
    "    for datasets in ['LAML','ACC','CHOL','BLCA','BRCA','CESC','COAD','UCEC','ESCA','GBM','HNSC','KICH','KIRC','KIRP','DLBC','LIHC','LGG','LUAD','LUSC','SKCM','MESO','UVM','OV','PAAD','PCPG','PRAD','READ','SARC','STAD','TGCT','THYM','THCA','UCS']:\n",
    "        if datasets in dataset_name:\n",
    "            continue\n",
    "        try:\n",
    "            # use download_and_parse_data to download the data\n",
    "            dataframe = download_and_parse_data(datasets)\n",
    "            # 把Status为0的dataframe赋值给Status0data\n",
    "            Status0data = pd.concat([Status0data, dataframe[dataframe['Status'] == 0]])\n",
    "            Status1data = pd.concat([Status1data, dataframe[dataframe['Status'] == 1]])\n",
    "        except:\n",
    "            print('Error:', datasets)\n",
    "            continue\n",
    "\n",
    "    # 混入对应比例的1\n",
    "    # Status0data = pd.concat([Status0data, Status1data.sample(len(Status0data)*rate1/(1-rate1))])\n",
    "    # int is needed\n",
    "    Status0data = pd.concat([Status0data, Status1data.sample(n=int(len(Status0data)*rate1/(1-rate1)),random_state=42)])\n",
    "    # 把所有的label 都设为0\n",
    "    Status0data['Status'] = 0\n",
    "    # 保存0的dataframe为pkl文件\n",
    "    pkl_file_path = './datasets/Status0data.pkl'\n",
    "    Status0data.to_pickle(pkl_file_path)\n",
    "\n",
    "    return Status0data\n",
    "\n",
    "def balance_data(Currentdata):\n",
    "    # Get the count of Status=0 and Status=1\n",
    "    current1 = Currentdata[Currentdata['Status'] == 1]\n",
    "    current0 = Currentdata[Currentdata['Status'] == 0]\n",
    "    \n",
    "    if len(current0) > len(current1):\n",
    "        print('0 is more than 1')\n",
    "        # No need to balance, return the original DataFrame\n",
    "        return Currentdata\n",
    "    else:\n",
    "        print('1 is more than 0')\n",
    "        # If 1 is more, randomly sample from Status=0 data and concatenate\n",
    "        Status0data = getStatus0data()\n",
    "        balanced_data = pd.concat([current1, Status0data.sample(n=len(current1) - len(current0),random_state=42)])\n",
    "        print('Currentdata.shape:', balanced_data.shape)\n",
    "        return balanced_data\n",
    "\n",
    "def balance_data2(Currentdata):\n",
    "    # Get the count of Status=0 and Status=1\n",
    "    current1 = Currentdata[Currentdata['Status'] == 1]\n",
    "    current0 = Currentdata[Currentdata['Status'] == 0]\n",
    "    \n",
    "    if len(current0) > len(current1):\n",
    "        print('0 is more than 1')\n",
    "        # No need to balance, return the original DataFrame\n",
    "        return Currentdata\n",
    "    else:\n",
    "        print('1 is more than 0')\n",
    "        # If 1 is more, randomly remove some Status=1 data and concatenate\n",
    "        balanced_data = pd.concat([current0, current1.sample(n=len(current0),random_state=42)])\n",
    "        print('Currentdata.shape:', balanced_data.shape)\n",
    "        return balanced_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == \"diff\":\n",
    "    dataA = download_and_parse_data(dataset_name[0])\n",
    "    dataB = download_and_parse_data(dataset_name[1])\n",
    "\n",
    "    A_up = dataA[dataA['Status'] == 1]\n",
    "    B_up = dataB[dataB['Status'] == 1]\n",
    "\n",
    "    B_up['Status'] = 0\n",
    "    df_transposed = pd.concat([A_up, B_up], ignore_index=True)\n",
    "elif method == \"normal\":\n",
    "    df_transposed = pd.DataFrame()\n",
    "    for name in dataset_name:\n",
    "        df_transposed = df_transposed.append(download_and_parse_data(name))\n",
    "\n",
    "# if balance_input, we need to balance the data using different datasets.\n",
    "if balance_input == 'true':\n",
    "    df_transposed = balance_data(df_transposed)\n",
    "    \n",
    "train_data = df_transposed.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数 准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义存储结果的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(columns=['method', 'model', 'n', 'acc', 'auc', 'f1', 'precision', 'recall', 'confusionmatrix','coef', 'coef_rounded', 'found_coef', 'multiple_times','intercept', 'miRNA_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可接受的权重的范围筛选器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allowed_numbers(max_n=25,allowed_factors=[1,2,3,4,5]):\n",
    "    # 100 以内的整数\n",
    "    result = range(1,100)\n",
    "    return result\n",
    "\n",
    "def is_all_in_list(list,n=25,allowed_factors=[2,3,5]):\n",
    "    for i in list:\n",
    "        if i not in get_allowed_numbers(n,allowed_factors):\n",
    "            return False\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分数分布图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels = ['得了', '没得']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distribution(ax, y_score, final_Status):                                 # 这个函数负责处理分数分布图这一个小图\n",
    "    ax.scatter(range(len(y_score)), y_score, c=final_Status, cmap='bwr', alpha=0.5)     # 画散点图，x轴是样本序号，y轴是分数，颜色是感染情况，颜色映射是蓝白红，透明度是0.5\n",
    "                                                                                        # x是样本序号：分开画，免得都黏在一起了，透明度的设置是一样的原因。\n",
    "    ax.set_title('分数分布图')\n",
    "    ax.set_xlabel('样本序号')\n",
    "    ax.set_ylabel('分数')\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']                                          # 中文乱码\n",
    "    plt.rcParams['axes.unicode_minus']=False                                            # 负号乱码\n",
    "    labels = plot_labels                                                                # 图例标签，这里是感染情况，以后如果换了的话得改。 #todo 将这个改为传入参数\n",
    "    ax.legend(labels, loc='upper right')                                                # 图例\n",
    "    ax.plot([0, len(y_score)], [0.5, 0.5], color='black', lw=1, linestyle='--')         # 以0.5为界，画一条虚线，表示分数大于0.5的是细菌感染，小于0.5的是病毒感染，这个以后也得改\n",
    "                                                                                        # 因为不同的模型（如linear regression 和 ridge regression）的判断标准不一样，所以这个界限也不一样。\n",
    "                                                                                        # 现在的做法是直接在y_score 上做手脚，还是……蛮不优雅的，但是先这样吧。\n",
    "                                                                                        # todo 将这个改为传入参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(ax, y_score, final_Status):\n",
    "    from sklearn.metrics import auc\n",
    "    fpr, tpr, thresholds = roc_curve(final_Status, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax.set_xlabel('False Positive Rate 假阳性率')\n",
    "    ax.set_ylabel('True Positive Rate 真阳性率')\n",
    "    ax.set_title('ROC 曲线（受试者工作特征曲线）')\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(ax, y_score, final_Status):\n",
    "    precision, recall, thresholds = precision_recall_curve(final_Status, y_score)\n",
    "    ax.plot(recall, precision, color='darkorange', lw=2, label='PR curve')\n",
    "    ax.set_xlabel('Recall 召回率')\n",
    "    ax.set_ylabel('Precision 准确率')\n",
    "    ax.set_title('PR 曲线（准确率-召回率曲线）')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(ax, y_score, final_Status):\n",
    "    y_pred = np.where(y_score > 0.5, 1, 0)\n",
    "    cnf_matrix = confusion_matrix(final_Status, y_pred)\n",
    "    ax.imshow(cnf_matrix, interpolation='nearest',cmap=plt.cm.Blues) # type: ignore\n",
    "    ax.set_title('Confusion Matrix 混淆矩阵')\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, plot_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, plot_labels)\n",
    "    thresh = cnf_matrix.max() / 2.\n",
    "    for i in range(cnf_matrix.shape[0]):\n",
    "        for j in range(cnf_matrix.shape[1]):\n",
    "            plt.text(j, i, cnf_matrix[i, j],horizontalalignment=\"center\",color=\"white\" if cnf_matrix[i, j] > thresh else \"black\",fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label 感染情况')\n",
    "    plt.xlabel('Predicted label 预测情况')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分数分布图、ROC曲线、PR曲线和混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(score, Status, title=\"分数分布图、ROC曲线、PR曲线和混淆矩阵\", feature=None, weight=None):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))                                                     # 规定画布大小为1200*1200px，分成2*2的4个小图\n",
    "    fig.suptitle(title, fontsize=16)                                                                    # 设置标题\n",
    "    plot_score_distribution(axs[0,0], score, Status)                                                    # 第一个小图（左上角）是分数分布图 这里的axs[0,0]是指第一行第一列的小图，注意计数是从0开始的。\n",
    "    plot_roc_curve(axs[0,1], score, Status)                                                             # 第二个小图（右上角）是ROC曲线\n",
    "    plot_pr_curve(axs[1,0], score, Status)                                                              # 第三个小图（左下角）是PR曲线\n",
    "    plot_confusion_matrix(axs[1,1], score, Status)                                                      # 第四个小图（右下角）是混淆矩阵\n",
    "    if feature or weight:                                                                               # 如果有feature或者weight的话，就在图的下方加上文字\n",
    "        try:\n",
    "            # add margin at bottom\n",
    "            fig.subplots_adjust(bottom=0.12)                                                            # 调整图的下边距，这样文字就不会挡住图了\n",
    "            # add feature and weight at the bottom\n",
    "            fig.text(0.5, 0.04, '使用feature:'+str(feature), ha='center', va='bottom', fontsize=12)     # 在图的下方加上文字，这里的0.5,0.04是指文字的位置，ha='center'是指文字居中，va='bottom'是指文字在底部，fontsize=12是指文字大小\n",
    "            fig.text(0.5, 0.02, '对应权重:'+str(weight), ha='center', va='bottom', fontsize=8)          # 同上\n",
    "        except:\n",
    "            pass                                                                                        # 如果出错了就算了\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 给定 基因 权重 和数据，计算分数并画图（可能需要修改）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选哪些基因比较有差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_columns(data, diff=2.0):\n",
    "    data_mean = data.groupby('Status').mean()\n",
    "    higher_columns = []\n",
    "    higher_times = []\n",
    "    for i in range(data_mean.shape[1]):\n",
    "        if  diff*data_mean.iloc[0,i] < data_mean.iloc[1,i]  or data_mean.iloc[0,i] > diff*data_mean.iloc[1,i]:\n",
    "            higher_columns.append(data_mean.columns[i])\n",
    "            # 0\n",
    "            if data_mean.iloc[0,i] == 0:\n",
    "                higher_times.append(1e10)\n",
    "            else:\n",
    "                higher_times.append(data_mean.iloc[1,i]/data_mean.iloc[0,i])\n",
    "    return higher_columns, higher_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "排序有差异的基因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_higher_columns(data, higher_columns, higher_times):\n",
    "    # sort the columns and times together\n",
    "    data_higher_times, data_higher_columns = zip(*sorted(zip(higher_times, higher_columns), reverse=True))\n",
    "    # convert data_higher_columns to list\n",
    "    data_higher_columns = list(data_higher_columns)\n",
    "    # add Status column to the top of data_higher_columns\n",
    "    data_higher_columns.append('Status')\n",
    "    # get the new data\n",
    "    data_higher = data[data_higher_columns]\n",
    "    return data_higher, data_higher_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_acc(test_acc, n, gene_symbol, coef):\n",
    "    best_acc = max(test_acc)\n",
    "    best_i = 0\n",
    "    i = 0\n",
    "    print('最好的准确率为：', best_acc)\n",
    "    for acc in test_acc:\n",
    "        if acc == best_acc:\n",
    "            print('='*60)\n",
    "            print('对应的特征数为：', n[i],'方法为：', methods[i // len(models)].__name__,'特征为：', gene_symbol[i])\n",
    "            print('模型为：', models[i % len(models)],'特征权重为：', coef[i])\n",
    "            weights_avg = np.mean(coef[i])\n",
    "            new_coefs = []\n",
    "            for j in range(len(coef[i])):\n",
    "                # new_coefs.append(10*(coef[i][j]/weights_avg)) 保留2位小数\n",
    "                # new_coefs.append(round(10*(coef[i][j]/weights_avg), 2)) type numpy.ndarray doesn't define __round__ method\n",
    "                new_coefs.append(np.round(10*(coef[i][j]/weights_avg), 2))\n",
    "            print('比例大致为：', new_coefs)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次性完成基因筛选和模型训练\n",
    "通过调控C，使用LogisticRegression or LinearSVC 一次性完成基因筛选和模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_to_color(percent: float) -> str:\n",
    "    assert 0 <= percent <= 1, \"Percent must be between 0 and 1\"\n",
    "    if percent >= 0.82:\n",
    "        hue = int(120 - (1 - percent) * 4000 / 3)\n",
    "    else:\n",
    "        hue = 0\n",
    "    saturation = 100  # 饱和度为100%\n",
    "    lightness = 50  # 亮度为50%\n",
    "    return f\"hsl({hue}, {saturation}%, {lightness}%)\"\n",
    "\n",
    "def colored_text(text: str, percent: float) -> str:\n",
    "    color = percent_to_color(percent)\n",
    "    return f'<span style=\"color: {color}\">{text}</span>'\n",
    "def evaluate_classifier(clf, method, model, X_train, y_train, X_test, y_test, params):\n",
    "    global final_results\n",
    "\n",
    "    acc_scores, auc_scores, f1_scores, precision_scores, recall_scores, n_features = [], [], [], [], [], []\n",
    "\n",
    "    for param in params:\n",
    "        clf.set_params(**param)\n",
    "        clf.fit(X_train, y_train)\n",
    "        nonzero_coef_indices = np.flatnonzero(clf.coef_)  # Indices where coef_ != 0\n",
    "        n = len(nonzero_coef_indices)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred = np.array([1 if i > 0.5 else 0 for i in y_pred], dtype=int)\n",
    "        \n",
    "        if 1 <= n <= 20:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            f2 = fbeta_score(y_test, y_pred, beta=2)\n",
    "            f05 = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            \n",
    "            # acc = clf.score(X_test, y_test) if method is lasso or elasticnet, score is not accuracy\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            coef = clf.coef_\n",
    "            # only use non-zero coef\n",
    "            coef = coef[coef != 0]\n",
    "            weights_avg = np.mean(np.abs(coef))\n",
    "            max_times = 100 / np.max(np.abs(coef))\n",
    "            multipletimes = [max_times]\n",
    "            found_coef = False\n",
    "            coef_roundeds = []\n",
    "            deltas = []\n",
    "            for multipletime in multipletimes:\n",
    "                coef_rounded_t = []\n",
    "                differences = []\n",
    "                for coef_i in coef:\n",
    "                    # 符号问题,先存符号\n",
    "                    coef_i_sign = coef_i / np.abs(coef_i)\n",
    "                    coef_i = np.abs(coef_i)\n",
    "                    coef_rounded_t.append(coef_i * multipletime * coef_i_sign)\n",
    "                    differences.append(0)  # 不再使用 allowed_numbers\n",
    "                delta = np.average(np.array(differences))\n",
    "                deltas.append(delta)\n",
    "                coef_roundeds.append(coef_rounded_t)\n",
    "\n",
    "            coef_rounded = coef_roundeds[np.argmin(deltas)]\n",
    "            delta = np.min(deltas)\n",
    "            mean_coef = np.mean(np.abs(coef_rounded))\n",
    "            times = mean_coef / weights_avg\n",
    "            if delta < 0.30: # 30%的平均误差\n",
    "                found_coef = True\n",
    "            if np.sum(coef_rounded) == 0:\n",
    "                balanced = True\n",
    "            else:\n",
    "                balanced = False\n",
    "\n",
    "            intercept = clf.intercept_\n",
    "\n",
    "            nonzero_coef_indices = np.flatnonzero(clf.coef_) # get the indices of non-zero coef\n",
    "            gene_id = list(X_train.columns[nonzero_coef_indices])\n",
    "\n",
    "            result = pd.DataFrame({\n",
    "                'method': [method],\n",
    "                'model': [model],\n",
    "                'n': [n],\n",
    "                'acc': [acc],\n",
    "                'auc': [auc],\n",
    "                'f1': [f1],\n",
    "                'f2': [f2],\n",
    "                'f05': [f05],\n",
    "                'precision': [precision],\n",
    "                'recall': [recall],\n",
    "                'confusionmatrix': [cm],\n",
    "                'coef': [coef.tolist()],\n",
    "                'coef_rounded': [coef_rounded],\n",
    "                'found_coef': [found_coef],\n",
    "                'delta': [delta],\n",
    "                'multiple_times': [times],\n",
    "                'intercept': [intercept.item()],\n",
    "                'balanced': [balanced],\n",
    "                'gene_id': [gene_id],\n",
    "            })\n",
    "\n",
    "            final_results = pd.concat([final_results, result], ignore_index=True) \n",
    "\n",
    "            # For plotting\n",
    "            n_features.append(n) \n",
    "            acc_scores.append(acc)\n",
    "            auc_scores.append(auc)\n",
    "            f1_scores.append(f1)\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "            if loglevel == 'debug':\n",
    "                result_text = \"\"\n",
    "                result_text += f'method: {method}, '\n",
    "                result_text += f'model: {model}, '\n",
    "                result_text += f'n: {n}, '\n",
    "                result_text += 'acc: ' + colored_text(f'{acc:.3f}', acc) + ', '\n",
    "                result_text += 'auc: ' + colored_text(f'{auc:.3f}', auc) + ', '\n",
    "                result_text += 'f1: ' + colored_text(f'{f1:.3f}', f1) + ', '\n",
    "                result_text += 'precision: ' + colored_text(f'{precision:.3f}', precision) + ', '\n",
    "                result_text += 'recall: ' + colored_text(f'{recall:.3f}', recall) + '<br>'\n",
    "                coef_text = 'coef: '+ coef_rounded.__str__() + 'Gene: ' + gene_id.__str__()\n",
    "                result_text += coef_text\n",
    "                # 对 coef 和 gene_id 同样这样处理\n",
    "                # 最后，一次性打印所有的结果\n",
    "                display(HTML(result_text))\n",
    "        if n > 22:\n",
    "            break\n",
    "\n",
    "    # Plotting inside the function\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(n_features, auc_scores, label='AUC')\n",
    "    plt.scatter(n_features, auc_scores)\n",
    "    plt.plot(n_features, f1_scores, label='F1 score')\n",
    "    plt.scatter(n_features, f1_scores)\n",
    "    plt.plot(n_features, precision_scores, label='Precision', linestyle='dashed')\n",
    "    plt.plot(n_features, recall_scores, label='Recall', linestyle='dashed')\n",
    "    plt.title('Performance metrics by number of features (' + method + ')')\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('Performance metrics')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_columns, higher_times = get_higher_columns(train_data, 2)\n",
    "print('高表达差异基因数目：', len(higher_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据倍率不同进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_higher, data_higher_times = sort_higher_columns(train_data, higher_columns, higher_times)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_higher.drop(['Status'], axis=1), data_higher['Status'], test_size=0.2, random_state=42)\n",
    "# X_train, y_train = sm.fit_resample(data_higher, data_higher['Status'])\n",
    "# smote\n",
    "# X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "# X_test, y_test= sm.fit_resample(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(train_data):\n",
    "    status = train_data.iloc[:, 0]\n",
    "    other_columns = train_data.iloc[:, 1:]\n",
    "    correlations = {}\n",
    "\n",
    "    for column in other_columns.columns:\n",
    "        data = other_columns[column]\n",
    "        corr, _ = pearsonr(status, data)\n",
    "        if not pd.isnull(corr):\n",
    "            correlations[column] = corr\n",
    "\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)[:] \n",
    "\n",
    "    correlation_df = pd.DataFrame(sorted_correlations, columns=['Column', 'Correlation'])\n",
    "    correlation_df = correlation_df.rename(columns={'Column': 'gene_id'})\n",
    "    correlation_df.to_csv('correlation_results.csv', index=False)\n",
    "    print(\"Correlation results saved to correlation_results.csv\")\n",
    "calculate_correlation(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练不同的模型，使用不同的特征选择方法，选择出特征后，使用不同的方法来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(columns=['method', 'model', 'n', 'acc', 'auc', 'f1', 'precision', 'recall', 'f2','f05', 'confusionmatrix','coef', 'coef_rounded', 'found_coef', 'multiple_times','delta','intercept','balanced', 'gene_id'])\n",
    "max_run_num = 600\n",
    "C_values = np.logspace(-9, 0, num=max_run_num, base=10)\n",
    "alpha_values = np.logspace(15, -5, num=max_run_num, base=10)\n",
    "\n",
    "# LinearSVC\n",
    "lsvc = LinearSVC(penalty=\"l1\", dual=False)\n",
    "lsvc_params = [{'C': C} for C in C_values]\n",
    "evaluate_classifier(lsvc, 'LinearSVC', 'lsvc', X_train, y_train, X_test, y_test, lsvc_params)\n",
    "# Elastic-Net\n",
    "#    encv = ElasticNetCV(l1_ratio=1-np.logspace(-3, -0.1, num=10, base=10), n_alphas=200, max_iter=5000, n_jobs=10, selection='cyclic')\n",
    "#    encv.fit(X_train, y_train)\n",
    "#    print('-'*60)\n",
    "#    print('ElasticNetCV')\n",
    "#    print('best l1_ratio:', encv.l1_ratio_)\n",
    "#    print('selected features:', X_train.columns[encv.coef_ != 0].values)\n",
    "#    print('n features:', encv.n_features_in_)\n",
    "en = ElasticNet(alpha=0.1, l1_ratio=0.75, max_iter=5000, selection='cyclic')\n",
    "en_params = [{'alpha': alpha} for alpha in alpha_values]\n",
    "evaluate_classifier(en, 'ElasticNet', 'en', X_train, y_train, X_test, y_test, en_params)\n",
    "# LogisticRegression\n",
    "#       lrcv = LogisticRegressionCV(penalty='l1', solver='liblinear', max_iter=10000, n_jobs=-1)\n",
    "#       lrcv.fit(X_train, y_train)\n",
    "#       print('-'*60)\n",
    "#       print('LogisticRegressionCV')\n",
    "#       print('best C:', lrcv.C_)\n",
    "# print('selected features:', X_train.columns[lrcv.coef_ != 0].values)\n",
    "# print('n features:', lrcv.n_features_in_)\n",
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_params = [{'C': C} for C in C_values]\n",
    "evaluate_classifier(lr, 'LogisticRegression', 'lr', X_train, y_train, X_test, y_test, lr_params)\n",
    "# Lasso\n",
    "#       lassocv = LassoCV(max_iter=5000, n_jobs=-1)\n",
    "#       lassocv.fit(X_train, y_train)\n",
    "#       print('-'*60)\n",
    "#       print('LassoCV')\n",
    "#       print('best alpha:', lassocv.alpha_)\n",
    "#       print('selected features:', X_train.columns[lassocv.coef_ != 0].values)\n",
    "#       print('n features:', lassocv.n_features_in_)\n",
    "lasso = Lasso(alpha=0.1, max_iter=5000)\n",
    "lasso_params = [{'alpha': alpha} for alpha in alpha_values]\n",
    "evaluate_classifier(lasso, 'lasso', 'lasso', X_train, y_train, X_test, y_test, lasso_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新排序\n",
    "sort_columns = ['found_coef','f1', 'auc',  'balanced','n', 'delta']\n",
    "sort_order = [False,False, False, False,True, True]\n",
    "\n",
    "# Sort the DataFrame\n",
    "results_sorted = final_results.sort_values(by=sort_columns, ascending=sort_order).reset_index(drop=True).copy()\n",
    "results_sorted_view = results_sorted.head(5).copy().drop(['coef','gene_id','intercept','multiple_times'], axis=1)\n",
    "# results_sorted_view\n",
    "\n",
    "# 复制 'final_results' 到 'final_results_unique' \n",
    "final_results_unique = results_sorted.copy()\n",
    "\n",
    "# 将 'coef_rounded' 和 'gene_id' 列转化为字符串\n",
    "final_results_unique['coef_rounded'] = final_results_unique['coef_rounded'].astype('str')\n",
    "final_results_unique['gene_id'] = final_results_unique['gene_id'].astype('str')\n",
    "\n",
    "# 删除重复行\n",
    "final_results_unique.drop_duplicates(subset=['coef_rounded', 'gene_id'], keep='first', inplace=True)\n",
    "final_results_unique['coef_rounded'] = final_results_unique['coef_rounded'].apply(ast.literal_eval)\n",
    "final_results_unique['gene_id'] = final_results_unique['gene_id'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce\n",
    "results_sorted_found_coef = final_results_unique[results_sorted['found_coef'] == True].reset_index(drop=True).copy()\n",
    "reperduce = []\n",
    "for i in range(len(results_sorted_found_coef)):\n",
    "    gene_ids = results_sorted_found_coef['gene_id'][i]\n",
    "    # check if gene_ids are in X_test columns\n",
    "    if set(gene_ids).issubset(set(X_test.columns)):\n",
    "        X_test_selected = X_test[gene_ids].copy()\n",
    "    else:\n",
    "        # modify gene_ids to match X_test columns\n",
    "        gene_ids = [col for col in gene_ids if col in X_test.columns]\n",
    "        X_test_selected = X_test[gene_ids].copy()\n",
    "    y_test_selected = y_test\n",
    "    # calculate score\n",
    "    coef = results_sorted_found_coef['coef_rounded'][i]\n",
    "    times = results_sorted_found_coef['multiple_times'][i]\n",
    "    intercept = results_sorted_found_coef['intercept'][i]\n",
    "    model_name = results_sorted_found_coef['model'][i]\n",
    "\n",
    "    scores = []\n",
    "    predictions = []\n",
    "    \n",
    "    # determine model and calculate scores and predictions\n",
    "    if model_name == 'LinearRegression':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'RidgeClassifier' or model_name == 'RidgeClassifierCV':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'LogisticRegression' or model_name == 'LogisticRegressionCV':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            z = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            score = 1 / (1 + np.exp(-z))\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'Lasso' or model_name == 'LassoCV':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'LinearSVC' or model_name == 'NuSVC':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'ElasticNet':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    elif model_name == 'SGDClassifier':\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "    else:\n",
    "        # handle SVC\n",
    "        for j in range(len(X_test_selected)):\n",
    "            score = np.dot(X_test_selected.iloc[j], coef) / times + intercept\n",
    "            prediction = 1 if score > 0.5 else 0\n",
    "            scores.append(score)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "    # calculate evaluation metrics\n",
    "    acc = accuracy_score(y_test_selected, predictions)\n",
    "    auc = roc_auc_score(y_test_selected, scores)\n",
    "    f1 = f1_score(y_test_selected, predictions)\n",
    "    precision = precision_score(y_test_selected, predictions)\n",
    "    recall = recall_score(y_test_selected, predictions)\n",
    "\n",
    "    # 能复现\n",
    "    if acc == results_sorted_found_coef['acc'][i] and f1 == results_sorted_found_coef['f1'][i]:\n",
    "        can_reproduce = True\n",
    "    else:\n",
    "        can_reproduce = False\n",
    "    # can_reproduce = True\n",
    "\n",
    "    # if not acc == results_sorted_found_coef['acc'][i]:\n",
    "    #     can_reproduce = False\n",
    "    #     # print results\n",
    "    #     print('Model:', model_name, 'n:', results_sorted_found_coef['n'][i],'acc:', acc, 'auc:', auc, 'f1:', f1, 'precision:', precision, 'recall:', recall)\n",
    "    #     print(acc)\n",
    "    #     print(results_sorted_found_coef['acc'][i])\n",
    "    \n",
    "    reperduce.append(can_reproduce)\n",
    "\n",
    "results_sorted_found_coef['reproduce'] = reperduce\n",
    "results_sorted_found_coef_view = results_sorted_found_coef[results_sorted_found_coef['reproduce'] == True][results_sorted_found_coef['n'] <= 15].copy().drop(['coef','intercept','multiple_times'], axis=1)\n",
    "results_sorted_found_coef_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for results_sorted_found_coef, draw the figure\n",
    "# Group the results by method and model\n",
    "# filter by n\n",
    "results_sorted_found_coef_n = results_sorted_found_coef[results_sorted_found_coef['n'] <= 25]\n",
    "groups = results_sorted_found_coef_n.groupby(['method'])\n",
    "methods = [method for method, _ in groups]\n",
    "# Create a figure with multiple subplots\n",
    "fig, axs = plt.subplots(1,len(methods), sharex=True, sharey=True, figsize=(20, 12))\n",
    "\n",
    "# Iterate over the subplots and plot the acc and roc values\n",
    "for i, method in enumerate(methods):\n",
    "    group = groups.get_group(method)\n",
    "    axs[i].scatter(group['n'], group['f1'], label='F1', color='blue')\n",
    "    axs[i].scatter(group['n'], group['auc'], label='auc', color='orange')\n",
    "    axs[i].set_xlabel('n')\n",
    "    axs[i].set_ylabel('score')\n",
    "    axs[i].set_ylim([0.95, 1.00]) # Set y-axis limits\n",
    "    axs[i].legend()\n",
    "    # Find the maximum acc and roc values for this method and model\n",
    "    max_acc_idx = group['f1'].idxmax()\n",
    "    max_auc_idx = group['auc'].idxmax()\n",
    "    max_acc = group.loc[max_acc_idx, 'f1']\n",
    "    max_auc = group.loc[max_auc_idx, 'auc']\n",
    "    max_n_acc = group.loc[max_acc_idx, 'n']\n",
    "    max_n_auc = group.loc[max_auc_idx, 'n']\n",
    "\n",
    "    # Add annotations with the maximum acc and roc values\n",
    "    axs[i].annotate(f'Max F1: {max_acc:.3f} (n={max_n_acc})',\n",
    "                        xy=(max_n_acc, max_acc),\n",
    "                        xytext=(max_n_acc, max_acc - 0.006),\n",
    "                        ha='center',\n",
    "                        arrowprops=dict(arrowstyle='->'))\n",
    "    axs[i].annotate(f'Max Roc: {max_auc:.3f} (n={max_n_auc})',\n",
    "                        xy=(max_n_auc, max_auc),\n",
    "                        xytext=(max_n_auc, max_auc + 0.006),\n",
    "                        ha='center',\n",
    "                        arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "    axs[i].set_title(f'{method} - selected genes')\n",
    "    # grid\n",
    "    axs[i].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导出保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calthreadhold( model_name, instances, times):\n",
    "    if model_name == 'LinearRegression':\n",
    "        threshold = (0.5 - instances) * times\n",
    "    elif model_name == 'RidgeClassifier' or model_name == 'RidgeClassifierCV':\n",
    "        threshold = (0.5 - instances) * times\n",
    "    elif model_name == 'LogisticRegression' or model_name == 'LogisticRegressionCV':\n",
    "            threshold = (0 - instances) * times\n",
    "    elif model_name == 'lasso' or model_name == 'LassoCV':\n",
    "        threshold = (0.5 - instances) * times\n",
    "    elif model_name == 'LinearSVC' or model_name == 'NuSVC':\n",
    "        threshold = (0.5 - instances) * times\n",
    "    elif model_name == 'ElasticNet':\n",
    "        threshold = (0.5 - instances) * times\n",
    "    elif model_name == 'SGDClassifier':\n",
    "        threshold = (0.5 - instances) * times\n",
    "    else:\n",
    "        threshold = (0.5 - instances) * times\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算f1值的60%分位数\n",
    "# quantile_60 = results_sorted_found_coef['f1'].quantile(0.6)\n",
    "# \n",
    "# # 过滤results_sorted_found_coef，只保留f1值在前60%的数据\n",
    "# filtered_results = results_sorted_found_coef[results_sorted_found_coef['f1'] >= quantile_60]\n",
    "# filtered_results\n",
    "filtered_results = results_sorted_found_coef[results_sorted_found_coef['reproduce'] == True].copy()\n",
    "filtered_results['threshold'] = filtered_results.apply(lambda x: calthreadhold(x['model'], x['intercept'], x['multiple_times']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel \n",
    "import openpyxl\n",
    "# 时间\n",
    "import time\n",
    "xsl_file = str(dataset_name) + time.strftime(\"%Y%m%d%H%M%S\", time.localtime()) + '.xlsx'\n",
    "# 检查文件是否存在\n",
    "import os\n",
    "if os.path.exists(xsl_file):\n",
    "    os.remove(xsl_file)\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "try:\n",
    "    ws = wb['Results']\n",
    "except KeyError:\n",
    "    ws = wb.create_sheet('Results')\n",
    "# remove all rows\n",
    "ws.delete_rows(1, ws.max_row)\n",
    "# write column name\n",
    "for col in range(len(filtered_results.columns)):\n",
    "    ws.cell(row=1, column=col+1, value=filtered_results.columns[col])\n",
    "# write data\n",
    "for i in range(len(filtered_results)):\n",
    "    for col in range(len(filtered_results.columns)):\n",
    "        # not NaN\n",
    "        if filtered_results.iloc[i, col] == '':\n",
    "            ws.cell(row=i+2, column=col+1, value='N/A')\n",
    "        else:\n",
    "            try:\n",
    "                ws.cell(row=i+2, column=col+1, value=filtered_results.iloc[i, col])\n",
    "            except ValueError:\n",
    "                ws.cell(row=i+2, column=col+1, value=str(filtered_results.iloc[i, col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个默认字典来存储每个列表的f1值的总和、计数、最大f1值和对应的其他值\n",
    "f1_dict = defaultdict(lambda: [0, 0, 0, float('inf'), 0, 0, 0, None])\n",
    "\n",
    "# 遍历filtered_results，更新f1_dict\n",
    "for _, row in filtered_results.iterrows():\n",
    "    key = tuple(row['gene_id'])\n",
    "    f1_dict[key][0] += row['f1']\n",
    "    f1_dict[key][1] += 1\n",
    "    if row['f1'] > f1_dict[key][2] or (row['f1'] == f1_dict[key][2] and row['intercept'] < f1_dict[key][3]):\n",
    "        f1_dict[key][2] = row['f1']\n",
    "        f1_dict[key][3] = row['intercept']\n",
    "        f1_dict[key][4] = row['coef_rounded']\n",
    "        f1_dict[key][5] = row['multiple_times']\n",
    "        f1_dict[key][6] = row['delta']\n",
    "        f1_dict[key][7] = row['model']\n",
    "\n",
    "# 创建一个空的DataFrame\n",
    "df = pd.DataFrame(columns=['List', 'Length', 'Count', 'Average F1 Score', 'Max F1 Score','Best Coef',  'Best Intercept', 'Best Multiple Times', 'Best Delta', 'Best Model'])\n",
    "\n",
    "# 将f1_dict的内容添加到DataFrame中\n",
    "for key, value in f1_dict.items():\n",
    "    df = df.append({'List': list(key), 'Length': len(key), 'Count': value[1], 'Average F1 Score': value[0] / value[1], 'Max F1 Score': value[2], 'Best Intercept': value[3], 'Best Coef': value[4], 'Best Multiple Times': value[5], 'Best Delta': value[6], 'Best Model': value[7]}, ignore_index=True)\n",
    "\n",
    "# 按照长度和出现次数进行排序\n",
    "df = df.sort_values(by=['Length', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = wb['Sheet1']\n",
    "except KeyError:\n",
    "    ws = wb.create_sheet('Sheet1')\n",
    "# remove all rows\n",
    "ws.delete_rows(1, ws.max_row)\n",
    "# write column name\n",
    "for col in range(len(df.columns)):\n",
    "    ws.cell(row=1, column=col+1, value=df.columns[col])\n",
    "# write data\n",
    "for i in range(len(df)):\n",
    "    for col in range(len(df.columns)):\n",
    "        # not NaN\n",
    "        if df.iloc[i, col] == '':\n",
    "            ws.cell(row=i+2, column=col+1, value='N/A')\n",
    "        else:\n",
    "            try:\n",
    "                ws.cell(row=i+2, column=col+1, value=df.iloc[i, col])\n",
    "            except ValueError:\n",
    "                ws.cell(row=i+2, column=col+1, value=str(df.iloc[i, col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.DataFrame()\n",
    "# count\n",
    "miRNAs = filtered_results['gene_id'].tolist()\n",
    "# 统计出现次数 miRNAs_count as a dict\n",
    "miRNAs_count = {}\n",
    "for miRNA in miRNAs:\n",
    "    # miRNA is a list\n",
    "    # Add length of miRNA to miRNA\n",
    "    for miRNA_ in miRNA:\n",
    "        if miRNA_ in miRNAs_count:\n",
    "            miRNAs_count[miRNA_] += 1\n",
    "        else:\n",
    "            miRNAs_count[miRNA_] = 1\n",
    "\n",
    "miRNAs_count = sorted(miRNAs_count.items(), key=lambda x: x[1], reverse=True)\n",
    "miRNAs_count = pd.DataFrame(miRNAs_count, columns=['gene_id', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = wb['Count']\n",
    "except KeyError:\n",
    "    ws = wb.create_sheet('Count')\n",
    "# remove all rows\n",
    "ws.delete_rows(1, ws.max_row)\n",
    "# write column name\n",
    "df = miRNAs_count\n",
    "for col in range(len(df.columns)):\n",
    "    ws.cell(row=1, column=col+1, value=df.columns[col])\n",
    "# write data\n",
    "for i in range(len(df)):\n",
    "    for col in range(len(df.columns)):\n",
    "        # not NaN\n",
    "        if df.iloc[i, col] == '':\n",
    "            ws.cell(row=i+2, column=col+1, value='N/A')\n",
    "        else:\n",
    "            try:\n",
    "                ws.cell(row=i+2, column=col+1, value=df.iloc[i, col])\n",
    "            except ValueError:\n",
    "                ws.cell(row=i+2, column=col+1, value=str(df.iloc[i, col]))\n",
    "# save\n",
    "# delete default sheet\n",
    "wb.remove(wb['Sheet'])\n",
    "\n",
    "wb.save(xsl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择的模型\n",
    "# 实际输入的模型\n",
    "select = results_sorted_found_coef[results_sorted_found_coef['reproduce'] == True][results_sorted_found_coef['n'] <=20].copy().iloc[0]\n",
    "feature = ['hsa-mir-21', 'hsa-mir-10b']\n",
    "weight = [100.0,100.0]\n",
    "\n",
    "times = select.multiple_times\n",
    "print('times =', times)\n",
    "intercept = select.intercept\n",
    "print('intercept =', intercept)\n",
    "model = select.model\n",
    "print('model =', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = feature.copy()\n",
    "ID.append(\"Status\")\n",
    "ID = np.array(ID).reshape(-1)\n",
    "ID.astype('str')\n",
    "testdata = pd.concat([X_test, y_test], axis=1)[ID]\n",
    "# testdata = train_data[ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对平均值都乘以权重\n",
    "weight = weight + [1]\n",
    "testdata_weight = testdata * weight\n",
    "weight = weight[:-1]\n",
    "testdata_weight['score'] = testdata_weight.sum(axis=1) / times + intercept\n",
    "if model == 'lr':\n",
    "    testdata_weight['score'] = testdata_weight['score'].apply(lambda x: 1 / (1 + np.exp(-x)))\n",
    "testdata_weight['prediction'] = testdata_weight['score'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "def plot_metrics(testdata_weight, feature, weight):\n",
    "    # 创建一个figure并添加4个子图（ax）\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    # 在第一个子图上绘制分数分布图\n",
    "    scatter = axs[0, 0].scatter(range(len(testdata_weight['score'])), testdata_weight['score'], c=testdata_weight['Status'], cmap='bwr', alpha=0.5)\n",
    "    axs[0, 0].plot([0, len(testdata_weight['score'])], [0.5, 0.5], color='black', lw=1, linestyle='--')\n",
    "    axs[0, 0].set_title('Score distribution')\n",
    "    legend1 = axs[0, 0].legend(['Threshold', 'Positive', 'Negative'], loc=\"upper right\")\n",
    "    axs[0, 0].add_artist(legend1)\n",
    "\n",
    "    # 在第二个子图上绘制ROC曲线\n",
    "    fpr, tpr, _ = roc_curve(testdata_weight['Status'], testdata_weight['score'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axs[0, 1].plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "    axs[0, 1].plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    axs[0, 1].set_xlim([0.0, 1.0])\n",
    "    axs[0, 1].set_ylim([0.0, 1.05])\n",
    "    axs[0, 1].set_xlabel('False Positive Rate')\n",
    "    axs[0, 1].set_ylabel('True Positive Rate')\n",
    "    axs[0, 1].set_title('Receiver operating characteristic')\n",
    "    axs[0, 1].legend(loc=\"lower right\")\n",
    "\n",
    "    # 在第三个子图上绘制PR曲线\n",
    "    precision, recall, _ = precision_recall_curve(testdata_weight['Status'], testdata_weight['score'])\n",
    "    axs[1, 0].step(recall, precision,color='darkorange', lw=2, where='post')\n",
    "    axs[1, 0].set_xlabel('Recall')\n",
    "    axs[1, 0].set_ylabel('Precision')\n",
    "    axs[1, 0].set_ylim([0.0, 1.05])\n",
    "    axs[1, 0].set_xlim([0.0, 1.0])\n",
    "    axs[1, 0].set_title('Precision-Recall curve')\n",
    "    axs[1, 0].legend(['PR curve'])\n",
    "\n",
    "    # 在第四个子图上绘制混淆矩阵\n",
    "    cm = confusion_matrix(testdata_weight['Status'], testdata_weight['prediction'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "    disp.plot(ax=axs[1, 1], cmap='Blues', colorbar=False)\n",
    "    axs[1, 1].set_title('Confusion Matrix')\n",
    "\n",
    "    # add margin at bottom\n",
    "    fig.text(0.5, 0.04, 'feature:'+str(feature), ha='center', va='bottom', fontsize=12)     # 在图的下方加上文字，这里的0.5,0.04是指文字的位置，ha='center'是指文字居中，va='bottom'是指文字在底部，fontsize=12是指文字大小\n",
    "    fig.text(0.5, 0.02, 'Weight:'+str(weight), ha='center', va='bottom', fontsize=12)          # 同上\n",
    "\n",
    "    fig.suptitle('Metrics and Feature Weights', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    # 得调整图的下边距，否则会显示不全\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_int = np.array(weight, dtype=int)\n",
    "plot_metrics(testdata_weight, feature, weight_int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
